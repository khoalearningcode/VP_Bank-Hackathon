import base64
import os
import re
import time
import sys
import cv2
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
import easyocr
import cv2
import numpy as np
from PIL import Image
import pandas as pd
import torch
import torch.nn.functional as F
from transformers import TrOCRProcessor, VisionEncoderDecoderModel, pipeline
from difflib import SequenceMatcher
from io import BytesIO
from paddleocr import PaddleOCR
import pytesseract
from PIL import Image, ImageDraw, ImageFont
from mmocr.apis import TextRecInferencer
import mmcv
import mmengine
import traceback
import concurrent.futures
import urllib.request
from pdf2image import convert_from_bytes
import fasttext
import json
import uuid
import hashlib
import datetime
import uuid
import hashlib
from typing import Dict, Any, Optional

if not hasattr(np, 'sctypes'):
    np.sctypes = {
        'int': [np.int8, np.int16, np.int32, np.int64],
        'uint': [np.uint8, np.uint16, np.uint32, np.uint64],
        'float': [np.float16, np.float32, np.float64],
        'complex': [np.complex64, np.complex128]
    }

url = "https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz"
urllib.request.urlretrieve(url, "lid.176.ftz")
model_lang_detect = fasttext.load_model("lid.176.ftz")
print("T·∫£i th√†nh c√¥ng lid.176.ftz")

print("mmcv version:", mmcv.__version__)
print("mmengine version:", mmengine.__version__)

print("MMOCR inference ready!")

trocr_processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-printed")
trocr_model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-printed")

SCHEMA_PATTERNS = {
    "company_name": [
        r"c√¥ng\s*ty",
        r"doanh\s*nghi·ªáp",
        r"t·∫≠p\s*ƒëo√†n",
    ],
    "company_type": [
        r"tr√°ch\s*nhi·ªám\s*h·ªØu\s*h·∫°n|t\.?n\.?h\.?h",
        r"c·ªï\s*ph·∫ßn|c\.?p",
        r"m·ªôt\s*th√†nh\s*vi√™n|m\.?t\.?v",
        r"h·ª£p\s*t√°c|h\.?t\.?x",
    ],
    "personal_info.id_type": [
        r"cccd|cmnd|cƒÉn\s*c∆∞·ªõc",
        r"h·ªô\s*chi·∫øu|passport",
    ],
    "personal_info.id_number": [
        r"\b\d{9,12}\b",
    ],
    "personal_info.full_name": [
        r"(√¥ng|b√†|ch·ªã|anh|nguy·ªÖn|tr·∫ßn|ph·∫°m|l√™|v≈©)\s+[A-Z√Ä√Å·∫¢√É·∫†√ÇƒÇƒê√ä√î∆†∆Ø][\w\s]+",
    ],
    "appointment_date": [
        r"\d{1,2}/\d{1,2}/\d{4}",
        r"ng√†y\s+\d{1,2}\s+th√°ng\s+\d{1,2}\s+nƒÉm\s+\d{4}",
    ],
    "signing_authority": [
        r"gi√°m\s*ƒë·ªëc|t·ªïng\s*gi√°m\s*ƒë·ªëc|ch·ªß\s*t·ªãch|ph√≥\s*gi√°m\s*ƒë·ªëc",
    ],
}

def load_text_detection_model():
    """Load PaddleOCR (DBNet/DBNet++) for detection only."""
    ocr_detector = PaddleOCR(
        use_doc_orientation_classify=False,
        use_doc_unwarping=False,
        use_textline_orientation=False)
   
    return ocr_detector
text_detector = load_text_detection_model()

def fasttext_detect_lang(text):
    """
    Detect language using FastText, return lang + confidence
    """
    if not text or not text.strip():
        return "unknown", 0.0

    text_norm = text.strip()

    try:
        labels, probs = model_lang_detect.predict(text_norm)
        lang = labels[0].replace("__label__", "")
        prob = probs[0]
    except Exception:
        lang, prob = "unknown", 0.0

    # N·∫øu confidence cao ‚Üí tin t∆∞·ªüng k·∫øt qu·∫£
    if prob >= 0.5:
        return lang, prob

    # --- Fallback khi FastText kh√¥ng ch·∫Øc ch·∫Øn ---
    clean_text = re.sub(r'[^\w\s]', '', text_norm, flags=re.UNICODE).strip()
    if not clean_text:
        return "unknown", 0.0

    # Ki·ªÉm tra ti·∫øng Vi·ªát
    if re.search(r'[√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ]', clean_text, re.IGNORECASE):
        return "vi", 0.8  # gi·∫£ ƒë·ªãnh confidence trung b√¨nh 0.8

    # N·∫øu ch·ªâ ch·ª©a k√Ω t·ª± Latin c∆° b·∫£n (kh√¥ng d·∫•u) ‚Üí coi l√† ti·∫øng Anh
    if re.fullmatch(r'[a-zA-Z0-9\s]+', clean_text):
        return "en", 0.8

    return "unknown", 0.0

def detect_text_regions(image):
    """Detect text boxes and recognition results using PaddleOCR predict()."""
    import tempfile
    tmp_path = tempfile.mktemp(suffix=".jpg")
    cv2.imwrite(tmp_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))

    start_time = time.time()
    results = text_detector.predict(tmp_path)
    runtime_total = round(time.time() - start_time, 3)

    df_records = []
    boxes = []
    box_times = []

    if not results:
        print("Kh√¥ng ph√°t hi·ªán v√πng ch·ªØ n√†o.")
        return boxes, pd.DataFrame()

    for res in results:
        # M·ªói ph·∫ßn t·ª≠ c√≥ th·ªÉ l√† object ho·∫∑c dict
        if hasattr(res, "res"):
            res_data = res.res
        elif isinstance(res, dict):
            res_data = res
        elif hasattr(res, "__dict__"):
            res_data = res.__dict__
        else:
            print("Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c ki·ªÉu k·∫øt qu·∫£:", type(res))
            continue

        # Debug th·ª≠
        print("üîë Keys trong res_data:", list(res_data.keys()))

        dt_polys = res_data.get("dt_polys")
        dt_scores = res_data.get("dt_scores", [])
        rec_texts = res_data.get("rec_texts", [])
        rec_scores = res_data.get("rec_scores", [])

        if dt_polys is None:
            print("Kh√¥ng c√≥ dt_polys trong res_data.")
            continue

        for i, poly in enumerate(dt_polys):
            try:
                box_start = time.time()
                points = np.array(poly, dtype=np.int32)
                x_min = int(np.min(points[:, 0]))
                y_min = int(np.min(points[:, 1]))
                x_max = int(np.max(points[:, 0]))
                y_max = int(np.max(points[:, 1]))
                score = float(dt_scores[i]) if i < len(dt_scores) else 1.0

                # N·∫øu c√≥ nh·∫≠n d·∫°ng ch·ªØ
                text = rec_texts[i] if i < len(rec_texts) else ""
                rec_conf = float(rec_scores[i]) if i < len(rec_scores) else score

                # Ph√°t hi·ªán ng√¥n ng·ªØ n·∫øu c√≥ text
                if text.strip():
                    lang, lang_conf = fasttext_detect_lang(text)
                else:
                    lang, lang_conf = "unknown", 0.0
                
                box_runtime = round(time.time() - box_start, 4)

                boxes.append({
                    "bbox": (x_min, y_min, x_max, y_max),
                    "score": rec_conf,
                    "runtime": box_runtime
                })

                df_records.append({
                    "doc_preprocessor_res": {
                        "angle": 0,
                        "input_path": None,
                        "model_settings": {
                            "use_doc_orientation_classify": False,
                            "use_doc_unwarping": False
                        },
                        "page_index": None
                    },
                    "dt_polys": [poly],
                    "input_path": tmp_path,
                    "model_settings": {
                        "use_doc_preprocessor": False,
                        "use_textline_orientation": False
                    },
                    "page_index": None,
                    "rec_boxes": [list(map(int, [x_min, y_min, x_max, y_max]))],
                    "rec_polys": [poly],
                    "rec_scores": [rec_conf],
                    "rec_texts": [text],
                    "return_word_box": False,
                    "text_det_params": {
                        "box_thresh": 0.5,
                        "limit_side_len": 960,
                        "limit_type": "max",
                        "max_side_limit": 960,
                        "thresh": 0.3,
                        "unclip_ratio": 2.0
                    },
                    "text_rec_score_thresh": 0,
                    "text_type": "general",
                    "textline_orientation_angles": [],
                    "runtime": [box_runtime],
                    "lang": lang,
                    "lang_conf": float(lang_conf)
                })

            except Exception as e:
                print(f"‚ùå L·ªói khi x·ª≠ l√Ω polygon: {e}")
                continue

    df_paddle = pd.DataFrame(df_records)
    print(f"‚úÖ PaddleOCR ph√°t hi·ªán {len(boxes)} v√πng ch·ªØ, th·ªùi gian: {runtime_total:.3f}s")
    return boxes, df_paddle

def draw_unicode_text(img, text, pos, color=(0, 255, 0), font_path="Roboto-Black.ttf", font_size=18):
    """
    V·∫Ω ch·ªØ Unicode (ti·∫øng Vi·ªát c√≥ d·∫•u) l√™n ·∫£nh OpenCV b·∫±ng Pillow.
    """
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    try:
        font = ImageFont.truetype(font_path, font_size)
    except:
        font = ImageFont.load_default()
    draw.text(pos, text, font=font, fill=color)
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

def extract_text(image):
    annotated = image.copy()
    ocr_texts, raw_text_list = [], []

    # --- Load c√°c m√¥ h√¨nh OCR ---
    reader = easyocr.Reader(['vi', 'en'], gpu=False)
    rec_inferencer = TextRecInferencer(model='sar')
    
    def timed_run(func, crop, name):
        start = time.time()
        result = func(crop)
        end = time.time()
        result["runtime"] = round(end - start, 3)
        result["model"] = name
        return result

    # ========== H√ÄM OCR ==========
    def run_ocr_tesseract(crop):
        try:
            # gray = preprocess_for_ocr(crop)
            config = '--oem 3 --psm 6 -l vie+eng'
            text = pytesseract.image_to_string(crop, config=config).strip()
            text = re.sub(r'\s+', ' ', text)
            conf = 0.5 if text else 0.0
            lang, lang_conf = fasttext_detect_lang(text) if text else ("unknown", 0.0)
            return {"text": text, "conf": conf, "lang": lang, "lang_conf": lang_conf}
        except Exception as e:
            print(f"‚ùå L·ªói Tesseract: {e}")
            return {"text": "", "conf": 0.0, "lang": "unknown", "lang_conf": 0.0}

    def run_ocr_easyocr(crop):
        try:
            # gray = preprocess_for_ocr(crop)
            results = reader.readtext(crop)
            if not results:
                return {"text": "", "conf": 0.0, "lang": "unknown", "lang_conf": 0.0}
            _, text, conf = max(results, key=lambda x: x[2])
            lang, lang_conf = fasttext_detect_lang(text) if text else ("unknown", 0.0)
            return {"text": text, "conf": conf, "lang": lang, "lang_conf": lang_conf}
        except Exception as e:
            print(f"‚ùå L·ªói EasyOCR: {e}")
            return {"text": "", "conf": 0.0, "lang": "unknown", "lang_conf": 0.0}

    # ========== C√îNG C·ª§ H·ªñ TR·ª¢ ==========
    def clean_text(t):
        t = re.sub(r'[^0-9A-Za-z√Ä-·ªπ\s.,:/%()+=-]', '', t)
        t = re.sub(r'\s+', ' ', t).strip()
        return t
    
    def has_good_spacing(text):
        return bool(re.search(r'[A-Za-z√Ä-·ªπ]+\s+[A-Za-z√Ä-·ªπ]+', text))

    def normalize_case(text):
        if not text.strip():
            return text

        letters = re.findall(r'[A-Za-z√Ä-·ªπ]', text)
        if not letters:
            return text  

        upper_count = sum(1 for c in letters if c.isupper())
        lower_count = len(letters) - upper_count

        if upper_count >= 0.7 * len(letters):
            return text.upper()

        elif lower_count >= 0.7 * len(letters):
            return text.lower()
        else:
            return text
    
    def text_similarity(a, b):
        a_clean = re.sub(r'[^A-Za-z√Ä-·ªπ0-9]', '', a.lower())
        b_clean = re.sub(r'[^A-Za-z√Ä-·ªπ0-9]', '', b.lower())
        ratio = SequenceMatcher(None, a_clean, b_clean).ratio()
        return ratio * (1 - abs(len(a_clean) - len(b_clean)) / max(len(a_clean), len(b_clean), 1))
 
    def choose_best_text(results):
        # --- L·∫•y t·∫•t c·∫£ text kh√¥ng r·ªóng ---
        texts = [r for r in results.values() if r["text"]]
        for r in texts:
            r["text"] = normalize_case(clean_text(r["text"]))

        if not texts:
            return "(r·ªóng)", 0.0, "unknown"

        # --- L·ªçc text c√≥ lang h·ª£p l·ªá ---
        valid_langs = [t for t in texts if t.get("lang") and t["lang"] != "unknown"]

        if valid_langs:
            # 1Ô∏è‚É£ Ch·ªçn candidate lang_conf cao nh·∫•t
            max_lang_conf = max(t.get("lang_conf", 0.0) for t in valid_langs)
            top_candidates = [t for t in valid_langs if t.get("lang_conf", 0.0) == max_lang_conf]
            
            if len(top_candidates) > 1:
                mean_conf = np.mean([t["conf"] for t in top_candidates])
                print(f"--- Mean conf to√†n c·ª•c: {mean_conf:.3f} ---")

                # --- B∆∞·ªõc 2: l·ªçc text d∆∞·ªõi mean_conf to√†n c·ª•c ---
                filtered_candidates = []
                for t in top_candidates:
                    if t["conf"] >= mean_conf:
                        filtered_candidates.append(t)
                        print(f"Gi·ªØ text to√†n c·ª•c: {t['text']} ({t['conf']:.2f}) >= {mean_conf:.2f}")
                    else:
                        print(f"Lo·∫°i text to√†n c·ª•c: {t['text']} ({t['conf']:.2f}) < {mean_conf:.2f}")
                if not filtered_candidates:
                    filtered_candidates = top_candidates
                    print("Kh√¥ng c√≤n text n√†o sau l·ªçc to√†n c·ª•c, fallback top_candidates c≈©")

                # --- B∆∞·ªõc 3: ph√¢n nh√≥m theo text chu·∫©n h√≥a ---
                def normalize_text_for_grouping(t):
                    s = t.lower()
                    return s

                grouped = {}
                for t in filtered_candidates:
                    key = normalize_text_for_grouping(t["text"])
                    grouped.setdefault(key, []).append(t)
                
                print(f"S·ªë nh√≥m sau ph√¢n nh√≥m: {len(grouped)}")
                for key, group in grouped.items():
                    group_mean_conf = np.mean([x["conf"] for x in group])
                    print(f" Nh√≥m '{key}' - mean conf nh√≥m: {group_mean_conf:.3f}")
                    for x in group:
                        if x["conf"] >= group_mean_conf:
                            print(f"   Gi·ªØ: {x['text']} ({x['conf']:.2f}) >= {group_mean_conf:.2f}")
                        else:
                            print(f"   Lo·∫°i: {x['text']} ({x['conf']:.2f}) < {group_mean_conf:.2f}")

                # --- B∆∞·ªõc 4: l·ªçc trong m·ªói nh√≥m theo group mean conf ---
                top_candidates = []
                for key, group in grouped.items():
                    group_mean_conf = np.mean([x["conf"] for x in group])
                    group_filtered = [x for x in group if x["conf"] >= group_mean_conf]
                    if group_filtered:
                        top_candidates.extend(group_filtered)

                print(f"Ng∆∞·ª°ng to√†n c·ª•c: {mean_conf:.2f}")
                print(f"S·ªë nh√≥m: {len(grouped)}, gi·ªØ l·∫°i {len(top_candidates)} text sau l·ªçc nh√≥m")

            # else:
            #     continue

            # 2Ô∏è‚É£ N·∫øu nhi·ªÅu candidate b·∫±ng nhau, l·ªçc theo spacing
            spaced_candidates = [t for t in top_candidates if has_good_spacing(t["text"])]
            pool = spaced_candidates if spaced_candidates else top_candidates

            # 3Ô∏è‚É£ Trong pool, ch·ªçn nh·ªØng text ƒë·ªìng b·ªô v·ªÅ case (to√†n upper ho·∫∑c to√†n lower)
            case_candidates = []
            for t in pool:
                letters = re.findall(r'[A-Za-z√Ä-·ªπ]', t["text"])
                if not letters:
                    continue
                upper_count = sum(1 for c in letters if c.isupper())
                lower_count = len(letters) - upper_count
                if upper_count == len(letters) or lower_count == len(letters):
                    case_candidates.append(t)

            if case_candidates:
                # --- ƒê·∫øm t·∫ßn su·∫•t xu·∫•t hi·ªán text ---
                text_counter = {}
                for t in case_candidates:
                    txt = t["text"]
                    if txt in text_counter:
                        text_counter[txt].append(t)
                    else:
                        text_counter[txt] = [t]

                # --- T√¨m text xu·∫•t hi·ªán nhi·ªÅu nh·∫•t ---
                max_count = max(len(v) for v in text_counter.values())
                most_common_texts = [v for v in text_counter.values() if len(v) == max_count]

                if len(most_common_texts) == 1:
                    # Ch·ªâ c√≥ 1 text ph·ªï bi·∫øn nh·∫•t
                    best = max(most_common_texts[0], key=lambda t: t["conf"])
                else:
                    # N·∫øu nhi·ªÅu text c√πng t·∫ßn su·∫•t, ch·ªçn conf cao nh·∫•t
                    best = max([t for group in most_common_texts for t in group], key=lambda t: t["conf"])
            else:
                # Kh√¥ng c√≥ case_candidates ‚Üí fallback conf cao nh·∫•t trong pool
                best = max(pool, key=lambda t: t["conf"])

            return best["text"], float(best["conf"]), best["lang"]

        # --- N·∫øu kh√¥ng c√≥ lang h·ª£p l·ªá, fallback theo ƒë·ªô gi·ªëng nhau ---
        groups = []
        for t in texts:
            found = False
            for g in groups:
                if text_similarity(g[0]["text"], t["text"]) > 0.88:
                    g.append(t)
                    found = True
                    break
            if not found:
                groups.append([t])

        best_group = max(groups, key=lambda g: len(g))
        best = max(best_group, key=lambda t: t["conf"])
        final_text = normalize_case(best["text"])

        try:
            lang, _ = fasttext_detect_lang(final_text)
        except:
            lang = "unknown"

        return final_text, float(best["conf"]), lang

    # ========== PH√ÅT HI·ªÜN V√ôNG CH·ªÆ ==========
    boxes, df_paddle  = detect_text_regions(image)
    if not boxes:
        print(" Kh√¥ng ph√°t hi·ªán v√πng ch·ªØ, fallback to√†n ·∫£nh.")
        boxes = [(0, 0, image.shape[1], image.shape[0])]

    # ========== CH·∫†Y OCR TR√äN T·ª™NG V√ôNG ==========
    for idx, box in enumerate(boxes, start=1):
        try:
            (x_min, y_min, x_max, y_max) = box if not isinstance(box, dict) else box["bbox"]
            pad = 0
            x_min, y_min = max(0, x_min - pad), max(0, y_min - pad)
            x_max, y_max = min(image.shape[1], x_max + pad), min(image.shape[0], y_max + pad)
            crop = image[y_min:y_max, x_min:x_max]

            rec_text, rec_conf, rec_lang, rec_lang_conf = "", 0.0, "unknown", 0.0
            if not df_paddle.empty:
                # T√¨m h√†ng c√≥ rec_boxes tr√πng bbox hi·ªán t·∫°i
                match = df_paddle[
                    df_paddle["rec_boxes"].apply(
                        lambda b: list(map(int, [x_min, y_min, x_max, y_max])) in b
                    )
                ]
                if not match.empty:
                    rec_text = match.iloc[0]["rec_texts"][0]
                    rec_conf = float(match.iloc[0]["rec_scores"][0])
                    rec_lang = match.iloc[0].get("lang", "unknown")
                    rec_lang_conf = float(match.iloc[0].get("lang_conf", 0.0))

            with concurrent.futures.ThreadPoolExecutor() as executor:
                futures = {
                    "tesseract": executor.submit(timed_run, run_ocr_tesseract, crop, "tesseract"),
                    "easyocr": executor.submit(timed_run, run_ocr_easyocr, crop, "easyocr"),
                    # "mmocr": executor.submit(timed_run, run_ocr_mmocr, crop, "mmocr"),
                    # "trocr": executor.submit(timed_run, run_ocr_trocr, crop, "trocr")
                }
                results = {k: f.result() for k, f in futures.items()}

            # results["paddleocr"] = {
            #     "text": rec_text,
            #     "conf": rec_conf,
            #     "lang": rec_lang,
            #     "lang_conf": rec_lang_conf,
            #     "runtime": 0.0,
            #     "model": "paddleocr"
            # }
            # ---- Ch·ªçn text t·ªët nh·∫•t ----
            best_text, best_conf, best_lang = choose_best_text(results)
            best_text = corrector(best_text, max_length=MAX_LENGTH)

            # üîß ƒê·∫£m b·∫£o best_text lu√¥n l√† chu·ªói (string)
            if isinstance(best_text, list):
                # Tr∆∞·ªùng h·ª£p corrector tr·∫£ v·ªÅ [{'generated_text': '...'}]
                if len(best_text) > 0 and isinstance(best_text[0], dict) and "generated_text" in best_text[0]:
                    best_text = best_text[0]["generated_text"]
                else:
                    best_text = " ".join(map(str, best_text))
            elif isinstance(best_text, dict) and "generated_text" in best_text:
                best_text = best_text["generated_text"]
            elif not isinstance(best_text, str):
                best_text = str(best_text)

            print(f"text ƒë√£ chu·∫©n h√≥a: {best_text}")

            log_entry = {
                
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "region_index": idx,
                "bbox": [x_min, y_min, x_max, y_max],
                "models": results,
                "best_text": best_text,
                "best_conf": best_conf,
                "best_lang": best_lang
            }
            # append_log(log_entry)

            color = (0, 255, 0) if best_text != "(r·ªóng)" else (0, 200, 255)
            annotated = draw_unicode_text(annotated, best_text, (x_min, max(0, y_min - 18)), color)
            cv2.rectangle(annotated, (x_min, y_min), (x_max, y_max), color, 2)

            ocr_texts.append({
                "bbox": {"x": x_min, "y": y_min, "width": x_max - x_min, "height": y_max - y_min},
                "models": results,
                "final_text": best_text,
                "final_conf": best_conf,
                "lang": best_lang
            })
            raw_text_list.append(best_text)

            print(f"\nV√πng {idx}:")
            for name, r in results.items():
                print(f"  {name:<12} ‚Üí {r['text']} ({r['conf']:.2f}) [lang={r.get('lang','unknown')}, lang_conf={r.get('lang_conf',0.0):.2f}]")

            print(f" Ch·ªçn: {best_text} ({best_conf:.2f}) [lang={best_lang}]")

        except Exception as e:
            print(f" L·ªói x·ª≠ l√Ω v√πng {idx}: {e}")
            traceback.print_exc()

    print(f"\nHo√†n t·∫•t OCR: {len(ocr_texts)} v√πng.")
    return " ".join(map(str, raw_text_list)), ocr_texts, annotated

MAX_LENGTH = 512
corrector = pipeline("text2text-generation", model="bmd1905/vietnamese-correction")

# --- H·∫±ng s·ªë Mapping ---
COMPANY_TYPE_MAPPING: Dict[str, str] = {
    # T·ª´ ƒë·∫ßy ƒë·ªß
    "C√îNG TY": "CT",
    "C√îNGTY": "CT",
    "TR√ÅCH NHI·ªÜM H·ªÆU H·∫†N": "TNHH",
    "M·ªòT TH√ÄNH VI√äN": "MTV",
    "HAI TH√ÄNH VI√äN": "HTV",
    "C·ªî PH·∫¶N": "CP",
    "PH√ÅT TRI·ªÇN": "PT",
    "TH∆Ø∆†NG M·∫†I": "TM",
    "D·ªäCH V·ª§": "DV",
    "ƒê·∫¶U T∆Ø": "ƒêT",
    # T·ª´ vi·∫øt t·∫Øt
    "CT": "CT",
    "TNHH": "TNHH",
    "MTV": "MTV",
    "HTV": "HTV",
    "CP": "CP",
    "PT": "PT",
    "TM": "TM",
    "DV": "DV",
    "ƒêT": "ƒêT",
    "TV": "TV", # T∆∞ V·∫•n (gi·∫£ ƒë·ªãnh)
    "XD": "XD", # X√¢y D·ª±ng (gi·∫£ ƒë·ªãnh)
    "VT": "VT", # V·∫≠n T·∫£i (gi·∫£ ƒë·ªãnh)
    "GP": "GP", # Gi·∫£i Ph√°p (gi·∫£ ƒë·ªãnh)
    "JSC": "JSC", # Joint Stock Company
    "CO LTD": "Co LTD", # Company Limited
    "HKD": "HKD" # H·ªô Kinh Doanh
}

# √Ånh x·∫° cho ID Type (Gi·∫•y t·ªù t√πy th√¢n)
ID_TYPE_MAPPING: Dict[str, str] = {
    "CƒÇN C∆Ø·ªöC C√îNG D√ÇN": "CCCD",
    "CCCD": "CCCD",
    "CH·ª®NG MINH NH√ÇN D√ÇN": "CMND",
    "CMND": "CMND",
    "H·ªò CHI·∫æU": "Passport",
    "H·ªå CHI·∫æU": "Passport", # L·ªói OCR th∆∞·ªùng g·∫∑p
    "PASSPORT": "Passport",
}

# --- H√†m normalize_appointment_text ---
def normalize_appointment_text(ocr_text: str) -> Dict[str, Any]:
    """
    √Ånh x·∫° text th√¥ t·ª´ OCR th√†nh c√°c tr∆∞·ªùng chu·∫©n h√≥a theo schema.
    """
    normalized_data = {
        "company_name": "",
        "company_type": "",
        "personal_info": {
            "id_type": "",
            "id_number": "",
            "full_name": ""
        },
        "appointment_date": {
            "day": 0,
            "month": 0,
            "year": 0
        },
        "signing_authority": ""
    }
    
    # 1. Chu·∫©n h√≥a c∆° b·∫£n to√†n b·ªô text th√¥ (ch·ªØ hoa v√† lo·∫°i b·ªè d·∫•u/k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng c·∫ßn thi·∫øt)
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát, gi·ªØ l·∫°i ch·ªØ c√°i, s·ªë, d·∫•u ti·∫øng Vi·ªát
    clean_text = re.sub(r'[^A-Z√Ä-·ª∏0-9\s/]', ' ', ocr_text.upper()) 
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()
    
    # 2. X·ª≠ l√Ω ID Type (CCCD/CMND/Passport)
    
    # T√¨m ki·∫øm c√°c t·ª´ kh√≥a ID Type trong text th√¥
    id_type_found: Optional[str] = None
    for full_form, enum_val in ID_TYPE_MAPPING.items():
        # T√¨m ki·∫øm c·ª•m t·ª´ ƒë·∫ßy ƒë·ªß/vi·∫øt t·∫Øt/l·ªói OCR, kh√¥ng ph√¢n bi·ªát ch·ªØ hoa
        if full_form in clean_text:
            id_type_found = enum_val
            break
            
    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p c√≥ nhi·ªÅu lo·∫°i gi·∫•y t·ªù li√™n ti·∫øp (CCCD/CMND/H·ªò CHI·∫æU)
    if id_type_found:
        normalized_data["personal_info"]["id_type"] = id_type_found
        
        # --- T√¨m s·ªë ID (th·ªß c√¥ng) ---
        # T·∫°m th·ªùi ch·ªâ t√¨m ki·∫øm m·ªôt chu·ªói s·ªë/k√Ω t·ª± sau t·ª´ kh√≥a ID.
        # ƒê√¢y l√† ph·∫ßn ph·ª©c t·∫°p nh·∫•t, c·∫ßn regex ph·ª©c t·∫°p h∆°n trong th·ª±c t·∫ø.
        try:
            # T√¨m ki·∫øm: T·ª´ kh√≥a ID (CMND/CCCD/H·ªò CHI·∫æU) + 0-5 kho·∫£ng tr·∫Øng/k√Ω t·ª±/d·∫•u hai ch·∫•m + s·ªë
            match_id = re.search(r'(' + '|'.join(ID_TYPE_MAPPING.keys()).replace(' ', '\s+') + r')[\s:\-\/]{0,5}([0-9A-Z]{7,15})', clean_text)
            if match_id:
                normalized_data["personal_info"]["id_number"] = match_id.group(2).strip()
        except:
            pass # B·ªè qua n·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c s·ªë ID
            
    # 3. X·ª≠ l√Ω Company Type (CT/TNHH/CP/...)
    
    # T√¨m ki·∫øm c·ª•m t·ª´ vi·∫øt t·∫Øt ho·∫∑c ƒë·∫ßy ƒë·ªß trong text th√¥
    company_type_found: Optional[str] = None
    for full_form, enum_val in COMPANY_TYPE_MAPPING.items():
        # Ki·ªÉm tra exact match (sau khi upper)
        # S·ª≠ d·ª•ng ranh gi·ªõi t·ª´ (\b) ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n "CT" v·ªõi "CHUTICH"
        if re.search(r'\b' + re.escape(full_form) + r'\b', clean_text):
            company_type_found = enum_val
            break
            
    if company_type_found:
        normalized_data["company_type"] = company_type_found
        
        # --- X·ª≠ l√Ω Company Name ---
        # T·∫°m th·ªùi ƒë·∫∑t t√™n c√¥ng ty l√† ph·∫ßn c√≤n l·∫°i c·ªßa d√≤ng c√≥ Company Type.
        # ƒê√¢y c≈©ng l√† ph·∫ßn ph·ª©c t·∫°p c·∫ßn NLP m·∫°nh h∆°n, t·∫°m th·ªùi l√†m ƒë∆°n gi·∫£n.
        try:
            # T√¨m ki·∫øm d√≤ng ch·ª©a Company Type v√† t√°ch ra.
            company_type_regex = r'\b(' + '|'.join(COMPANY_TYPE_MAPPING.keys()).replace(' ', '\s+') + r')\b'
            
            # T√¨m ki·∫øm d√≤ng ch·ª©a lo·∫°i h√¨nh c√¥ng ty
            lines = ocr_text.split('\n')
            for line in lines:
                if re.search(company_type_regex, line.upper()):
                    # L·∫•y text tr∆∞·ªõc ho·∫∑c sau t·ª´ kh√≥a lo·∫°i h√¨nh c√¥ng ty
                    
                    # Gi·ªØ nguy√™n case cho Company Name
                    parts = re.split(company_type_regex, line, flags=re.IGNORECASE)
                    
                    if len(parts) >= 3:
                        # V√≠ d·ª•: "C√îNG TY ABC TNHH" -> [ '', 'C√îNG TY', ' ABC ', 'TNHH', '']
                        # Gh√©p ph·∫ßn tr∆∞·ªõc v√† sau lo·∫°i h√¨nh (v√≠ d·ª•: ' C√îNG TY ABC TNHH' -> 'C√îNG TY ABC')
                        
                        # Lo·∫°i b·ªè lo·∫°i h√¨nh c√¥ng ty v√† c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát
                        name_part = parts[0] + parts[2]
                        name_part = re.sub(r'\b(' + '|'.join(COMPANY_TYPE_MAPPING.keys()).replace(' ', '\s+') + r')\b', '', name_part, flags=re.IGNORECASE).strip()
                        
                        # Gi·∫£ ƒë·ªãnh company name l√† ph·∫ßn c√≤n l·∫°i c·ªßa d√≤ng
                        if name_part:
                            normalized_data["company_name"] = name_part
                            break
        except:
            pass

    # 4. X·ª≠ l√Ω Appointment Date
    
    # T·∫°m th·ªùi t√¨m ki·∫øm theo format D/M/Y ho·∫∑c D-M-Y ho·∫∑c D.M.Y
    try:
        match_date = re.search(r'(\d{1,2})[\/\-\.](\d{1,2})[\/\-\.](\d{4})', clean_text)
        if match_date:
            day, month, year = map(int, match_date.groups())
            if 1 <= day <= 31 and 1 <= month <= 12 and 1900 <= year <= datetime.date.today().year + 1:
                normalized_data["appointment_date"]["day"] = day
                normalized_data["appointment_date"]["month"] = month
                normalized_data["appointment_date"]["year"] = year
    except:
        pass
        
    # 5. X·ª≠ l√Ω Full Name (th·ªß c√¥ng) v√† Signing Authority
    
    # Ph·∫ßn n√†y c·ª±c k·ª≥ kh√≥ v√¨ kh√¥ng c√≥ c·∫•u tr√∫c c·ªë ƒë·ªãnh. C·∫ßn m√¥ h√¨nh NER.
    # T·∫°m th·ªùi b·ªè qua ph·∫ßn t√¨m ki·∫øm Full Name v√† Signing Authority cho ƒë·∫øn khi c√≥ c·∫•u tr√∫c r√µ r√†ng.
    
    return normalized_data

def extract_text_v2(image, user_id="user_001"):
    """
    Ch·∫°y OCR (Tesseract + EasyOCR + PaddleOCR) v√† tr·∫£ v·ªÅ JSON theo schema AppointmentDecisionRaw.
    """
    # --- 1Ô∏è‚É£ T√≠nh hash ·∫£nh ---
    image_hash = "fake_hash"

    # --- 2Ô∏è‚É£ Ch·∫°y OCR ---
    
    # Gi·∫£ ƒë·ªãnh k·∫øt qu·∫£ OCR th√¥ cho v√≠ d·ª•
    text_all = "C·ªòNG HO√Ä X√É H·ªòI CH·ª¶ NGHƒ®A VI·ªÜT NAM. C√îNG TY TNHH X√¢y D·ª±ng ABC. Ng∆∞·ªùi ƒë·∫°i di·ªán: NGUY·ªÑN VƒÇN A. CCCD s·ªë 012345678901. Ng√†y k√Ω: 15/09/2024. Ch·ª©c danh: T·ªîNG GI√ÅM ƒê·ªêC"
    ocr_regions = [] 
    annotated = None 

    # --- 3Ô∏è‚É£ Gom text theo model ---
    tesseract_texts = []
    easyocr_texts = []
    paddle_boxes = []

    # Gi·∫£ ƒë·ªãnh ocr_regions ƒë√£ ƒë∆∞·ª£c ƒëi·ªÅn...
    tesseract_text = " ".join(tesseract_texts).strip() or text_all
    easyocr_text = " ".join(easyocr_texts).strip() or text_all 

    # --- 4Ô∏è‚É£ Chu·∫©n h√≥a th√¥ng tin ---
    # G·ªåI H√ÄM CHU·∫®N H√ìA M·ªöI
    normalized = normalize_appointment_text(text_all)

    # --- 5Ô∏è‚É£ T·∫°o JSON k·∫øt qu·∫£ ---
    result = {
        "_id": f"dec_{datetime.datetime.utcnow().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:4]}",
        "user_id": user_id,
        "image_hash": image_hash,
        "ocr_raw": {
            "paddle_boxes": paddle_boxes,
            "tesseract_text": tesseract_text,
            "easyocr_text": easyocr_text
        },
        "normalized": normalized,
        "status": "pending",
        "created_at": datetime.datetime.utcnow().isoformat() + "Z"
    }

    return result, annotated

def pdf_to_images(content: bytes, dpi=150):
    """
    Chuy·ªÉn PDF bytes th√†nh danh s√°ch ·∫£nh numpy (m·ªói trang m·ªôt ·∫£nh)
    """
    pages = convert_from_bytes(content, dpi=dpi)
    images = [np.array(p.convert("RGB")) for p in pages]
    return images

def process_image(content: bytes, filename: str):
    ext = os.path.splitext(filename)[1].lower()
    
    if ext == ".pdf":
        print("üìÑ Ph√°t hi·ªán file PDF, chuy·ªÉn sang ·∫£nh...")
        images = pdf_to_images(content)
        results = []

        for idx, img_array in enumerate(images, start=1):
            print(f"\n--- X·ª≠ l√Ω trang {idx}/{len(images)} ---")
            text, details, annotated = extract_text(img_array)
            _, buffer = cv2.imencode(".jpg", annotated)
            annotated_b64 = base64.b64encode(buffer).decode('utf-8')
            results.append({
                "page": idx,
                "text": text,
                "details": details,
                "annotated_image": annotated_b64
            })
        return results

    elif ext in [".jpg", ".jpeg", ".png", ".bmp", ".tiff"]:
        print("üñºÔ∏è Ph√°t hi·ªán file ·∫£nh, x·ª≠ l√Ω tr·ª±c ti·∫øp...")
        img = Image.open(BytesIO(content)).convert("RGB")
        img_array = np.array(img)
        text, details, annotated = extract_text(img_array)
        _, buffer = cv2.imencode(".jpg", annotated)
        annotated_b64 = base64.b64encode(buffer).decode('utf-8')
        return [{
            "page": 1,
            "text": text,
            "details": details,
            "annotated_image": annotated_b64
        }]
    
    else:
        raise ValueError(f"Kh√¥ng h·ªó tr·ª£ ƒë·ªãnh d·∫°ng: {ext}")

def compute_image_hash(image_array: np.ndarray) -> str:
    """T√≠nh SHA256 hash c·ªßa ·∫£nh numpy array (RGB)."""
    # ƒê·∫£m b·∫£o ƒë·ªãnh d·∫°ng nh·∫•t qu√°n
    if image_array.dtype != np.uint8:
        image_array = image_array.astype(np.uint8)
    # Chuy·ªÉn sang bytes theo th·ª© t·ª± c·ªë ƒë·ªãnh
    img_bytes = image_array.tobytes()
    return "sha256:" + hashlib.sha256(img_bytes).hexdigest()

def build_appointment_decision_json(
    image_array: np.ndarray,
    ocr_results: list,  # danh s√°ch t·ª´ `extract_text()`: m·ªói ph·∫ßn t·ª≠ l√† dict c√≥ "models", "bbox", ...
    user_id: str = "user_001",
    doc_id: str = None
) -> dict:
    """
    X√¢y d·ª±ng JSON theo schema collection_appointment_decisions.
    Ch·ªâ l∆∞u OCR raw, normalized ƒë·ªÉ tr·ªëng (s·∫Ω ƒë∆∞·ª£c ƒëi·ªÅn sau b·ªüi BERT).
    """
    # 1. T·∫°o ID
    if doc_id is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        doc_id = f"dec_{timestamp}_{str(uuid.uuid4())[:8]}"

    # 2. T√≠nh image hash
    img_hash = compute_image_hash(image_array)

    # 3. Tr√≠ch xu·∫•t text t·ª´ t·ª´ng engine
    tesseract_texts = []
    easyocr_texts = []
    paddle_boxes = []

    for region in ocr_results:
        bbox = region["bbox"]
        x, y, w, h = bbox["x"], bbox["y"], bbox["width"], bbox["height"]
        paddle_boxes.append({
            "points": [[x, y], [x + w, y], [x + w, y + h], [x, y + h]],
            "bbox": [x, y, w, h]
        })

        models = region.get("models", {})
        tess = models.get("tesseract", {}).get("text", "")
        easy = models.get("easyocr", {}).get("text", "")
        if tess:
            tesseract_texts.append(tess)
        if easy:
            easyocr_texts.append(easy)

    ocr_raw = {
        "paddle_boxes": paddle_boxes,
        "tesseract_text": "\n".join(tesseract_texts),
        "easyocr_text": "\n".join(easyocr_texts)
    }

    # 4. X√¢y d·ª±ng JSON
    output = {
        "_id": doc_id,
        "user_id": user_id,
        "image_hash": img_hash,
        "ocr_raw": ocr_raw,
        "normalized": {},  # ƒë·ªÉ tr·ªëng, s·∫Ω ƒë∆∞·ª£c BERT ƒëi·ªÅn sau
        "status": "pending",
        "created_at": datetime.utcnow().isoformat() + "Z"
    }

    return output
 
def main():
    path = r"./QDBN1.pdf"

    if not os.path.exists(path):
        print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {path}")
        return

    with open(path, "rb") as f:
        content = f.read()

    ext = os.path.splitext(path)[1].lower()
    base_name = os.path.splitext(os.path.basename(path))[0]

    if ext == ".pdf":
        print("üìÑ Ph√°t hi·ªán file PDF, chuy·ªÉn sang ·∫£nh...")
        images = pdf_to_images(content)

        for idx, img in enumerate(images, start=1):
            print(f"\n--- X·ª≠ l√Ω trang {idx}/{len(images)} ---")

            # ch·∫°y OCR tr·ª±c ti·∫øp
            text, details, annotated = extract_text(cv2.cvtColor(img, cv2.COLOR_RGB2BGR))

            # l∆∞u annotate
            preview_path = f"annotated_page_{idx}.jpg"
            cv2.imwrite(preview_path, cv2.cvtColor(annotated, cv2.COLOR_RGB2BGR))
            print(f"‚úÖ ·∫¢nh ch√∫ th√≠ch OCR trang {idx} ƒë√£ l∆∞u t·∫°i: {preview_path}")

            # build JSON
            json_data = build_appointment_decision_json(
                image_array=np.array(img),
                ocr_results=details,
                user_id="user_001",
                doc_id=f"dec_{base_name}_page{idx}"
            )
            json_path = f"appointment_decision_{base_name}_page{idx}.json"
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            print(f"‚úÖ ƒê√£ l∆∞u JSON schema trang {idx} v√†o: {json_path}")
            
    else:
        print(f"‚ö†Ô∏è Kh√¥ng h·ªó tr·ª£ ƒë·ªãnh d·∫°ng: {ext}")

if __name__ == "__main__":
    main()
